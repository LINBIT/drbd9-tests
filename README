DRBD 9 Test Suite
=================

This mainly is a test suite for version 9 of DRBD.  Some of the tests are
compatible with version 8 of DRBD as well, though.

The basic architecture of the test suite consists of a coordinator node and a
number of target nodes.  The test coordinator first opens ssh connections to
each of the target nodes to send the target nodes commands and collect results.
Then it prepares the target nodes for the actual test by creating test volumes
and a drbd configuration.  Once that is done, it sends the test nodes a series
of commands as defined by the test case, and waits for specific changes to
occur.  For example, it can tell the test cluster to bring itself up, and wait
until all nodes are fully connected.  All the test results are collected on the
coordinator.  Finally, when a test ends, it attempts to clean up any changes
made on the target nodes.

The test scripts themselves are relatively simple shell scripts, with
additional utilities for running commands remotely and for scanning log files
for events.


Requirements
------------

Running the test suite requires the following components:

 * One test coordinator node.  The basic test infrastructure can be run as an
   unprivileged user.

 * Typically two or more nodes to use as the test cluster.  The nodes must be
   accessible over ssh without password prompt. (This is best configured by
   putting the ssh public key of the user running the test suite into
   ~root/.ssh/authorized_keys on each test node.)

 * The DRBD 9 kernel module and user-space utilities must be installed on all
   test nodes.

 * The logscan utility (http://git.drbd.org/logscan.git/) must be installed on
   the test coordinator.

 * iptables on all test nodes.

 * For some tests, fio, the "Flexible I/O Tester" (https://github.com/axboe/fio).

 * The target components of the test suite (directory target/) must be built
   once, and then installed on all test nodes (for example: "make" on the test
   coordinator, then mount the test suite directory on all test nodes and "make
   install" on each of them).

 * On each test node, all test volumes are currently created inside an LVM
   volume group called "scratch".  (The volume group name can be overridden
   with the --volume-group=<name> option.)


Individual Test Runs
--------------------

Each individual test run is assigned a job name which consists of the base name
of the test, with a timestamp appended: the tests/connect script would log into
directory log/connect-20131015-173644/, for example.  The following files can
be found in the log directory:

  drbd.conf: the DRBD configuration file used on all nodes.

  events-<node-name>: The output of "drbdsetup events" on each node.  These
    event traces are used for checking for state transitions.

  test.log: The output of the test script (both standard output and standard
    error combined).

  <node-name>.log: The syslog of each node.

  .*.pos: positions of the last matches in the events-* files. The logscan
    utility uses these to keep trck of the current scanning positions.

  console-<node-name>: the serial console output of each node (if the
    --console or --vconsole options are used).


The most important options supported by the test scripts are:

  --node=<node-name> or just <node-name>: The name of one of the test nodes.
    Depending on the test, one, two, or more nodes are required.

  --disk=<size>, --meta=<size>: Create a volume of the specified size on each
    test node.  If the --meta option is not used, the volumes are created with
    internal metadata; otherwise, the data and metadata are split.
  
  --verbose: Log more information while running the test.

  --debug: Include some information for debugging the test suite.

All --disk and --meta options specified before the first --node option apply to
all nodes.  Any --disk or --meta options specified after a --node option apply
only to that node.

In addition, the test scripts can be run with "bash -x" for a complete
execution trace.


Batched Test Runs
-----------------

In addition to running test scripts individually, the test suite defines job
files, each of which runs an individual test case with specific parameters, on
a specific number of target nodes.  The lib/run-jobs script, when passed a list
of target nodes and a list of job files, will run each of the jobs on (a subset
of) the target nodes.


Virtual Machines
----------------

 * When testing virtual machines and logging their serial consoles, the virtual
   machine names must match the names under which the nodes are reachable via
   ssh.


Serial Consoles
---------------

 * Logging of serial console output is supported via the --console=/dev/...
   option.  The file permissions of that device must be set to allow read
   access to he user running the test suite.

 * Logging the serial console output of virtual machines running on the test
   coordinator is supported via the --vconsole option.  This requires that
   the virtual machine is visible by the current user in virsh.  The name of
   the virtual machine must be the same as the name used for accessing the node
   via ssh.


Examples
--------

  tests/connect tick trick track
  tests/initial-sync --verbose --disk=100M jekyll hyde
  lib/run-jobs maja willi flip max alexander -- jobs/*
