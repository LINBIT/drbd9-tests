#! /usr/bin/env python3

# This test creates a 3 node setup with quorum enables and simulates a
# crash of the primary. Then a other node gets promoted, but not a single
# bit is written after promotion.
# The resyncs after re-integration leave one node with an Outdated disk
# frequently. 10 retries seem to be enough to trigger this bug.
#
# This behaviour is reproducable up to (including) 9.0.25

import time
from python import drbdtest
from python.drbdtest import verbose
from subprocess import CalledProcessError

resource = drbdtest.setup(nodes=3)
resource.resource_options = 'quorum majority;'
resource.add_disk('10M')
resource.up_wait()
verbose('* Make up-to-date data available.')
resource.skip_initial_sync()
resource.forbidden_patterns.remove(r'connection:NetworkFailure')

[source_n, primary_n, secondary_n] = resource.nodes

def simulate_node_crash(node):
    c = resource.connections.from_node(node)
    c.disconnect(force=True)
    node.down()
    node.run(['drbdadm', 'apply-al', resource.name])

    # setting UUID_FLAG_CRASHED_PRIMARY UUID_FLAG_PRIMARY_LOST_QUORUM
    node.run(['drbdmeta', node.disks[0].minor, 'v09', node.disks[0].disk, 'internal',
              '--node-id=1', 'set-gi', ':::1::::1', '--force'])

    # Setting some bits in the bitmap, like entries in the activity log would do
    node.run(['/bin/bash', '-c', 'drbdadm dump-md %s > /tmp/md.txt' % (resource.name)])
    node.run(['sed', '-i', '-E',
              's/([0-9]+) times 0x0000000000000000/echo \\"0xFFFFFFFFFFFFFFFF; $((\\1-1)) times 0x0000000000000000; \\"/ge',
              '/tmp/md.txt'])
    node.run(['sed', '-i', '1d', '/tmp/md.txt'])
    node.run(['drbdmeta', node.disks[0].minor, 'v09', node.disks[0].disk, 'internal',
              'restore-md', '/tmp/md.txt', '--force'])
    node.run(['rm', '/tmp/md.txt'])

    # Modify the first 1k on the backing device
    node.run(['dd', 'if=/dev/urandom', 'of=%s' % (node.disks[0].disk), 'bs=1024', 'count=1', 'oflag=direct'])


def md5sum_first_1k(node):
    return node.run(['/bin/bash', '-c',
                     'dd if=%s bs=1024 iflag=direct count=1 | md5sum' % (node.disks[0].disk)],
                    return_stdout=True)


def test():
    witnessed_resync_from_primary = False
    source_n.primary()
    source_n.volumes[0].write(count=1, bs=1024, flags=['oflag=direct'])
    md5sum = md5sum_first_1k(source_n)

    simulate_node_crash(source_n)
    primary_n.primary()
    source_n.up()

    pd = resource.peer_devices.from_node(secondary_n).to_node(source_n)[0]
    pd.event(r'peer-device .* replication:SyncTarget')
    ev = secondary_n.volumes[0].event(r'device .* disk:(UpToDate|Outdated)')
    pd.event(r'peer-device .* replication:Established')
    if ev[0][0] == 'Outdated':
        # Expecting a second resync from the primary
        try:
            secondary_n.volumes[0].event(r'device .* disk:Inconsistent', timeout=5)
        except CalledProcessError:
            raise Exception('Sync Target is Outdated after resync')

        ev = secondary_n.volumes[0].event(r'device .* disk:UpToDate')
        witnessed_resync_from_primary = True

    verify_md5sum = md5sum_first_1k(secondary_n)
    if md5sum != verify_md5sum:
        raise Exception('Sync Target data mismatch: %s %s' % (md5sum, verify_md5sum))

    primary_n.secondary()
    return witnessed_resync_from_primary
#
for i in range(10):
    if test():
        break

resource.down()
resource.rmmod()

verbose(" * Verified that resync from primary happens. It took %d runs." % (i+1))
