#! /usr/bin/env python3
### vmshed: { "vms_all": [3], "vms_ci": [3], "drbd_version_min": "9.0", "variants_add": ["raw", "zfs"] }

# This test simultes the initial full sync from a node that is forced to
# consider itself as up-to-date.
# While the resync runs one of the sync-targets promotes and demotes, and the
# cluster is taken down and brought up again.
#
# Up to (including) drbd-9.0.25 this led to an clear-of-bitmap while the
# initial resync runs. It tiggers a log message ("expected n_oos"...) and often
# fails to correctly complete the sync after down and up.

from python import drbdtest
from python.drbdtest import connections, log, peer_devices
from python import datatools
from subprocess import CalledProcessError

resource = drbdtest.setup_resource(nodes=3)
resource.disk_options = 'c-max-rate 2M;'
resource.add_disk('10M')

source_n = resource.nodes[0]

resource.nodes.drbdadm(['--force', 'create-md', resource.name])
resource.nodes.new_resource()
resource.nodes.new_minor()
resource.nodes.attach()

source_n.primary(force=True)
source_n.volumes.write(size='100%', bs='64K')
source_n.secondary()

resource.nodes.adjust()

source_pds = peer_devices(source_n)
target_pds = peer_devices(to_node=source_n)

source_pds.event(r'peer-device .* replication:SyncSource')
target_pds.event(r'peer-device .* replication:SyncTarget')

while True:
    done = source_pds.event(r'peer-device .* done:([0-9.]+)')
    if float(done[0][0]) > 1.0 and float(done[1][0]) > 1.0:
        log(done)
        break

for n in resource.nodes[0:2]:
    n.volumes.write()

resource.down()
resource.up()

source_pds.event(r'peer-device .* replication:Established peer-disk:UpToDate')
target_pds.event(r'peer-device .* replication:Established')

datatools.verify_data(resource.nodes)

resource.down()
resource.cluster.teardown()
