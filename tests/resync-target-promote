#! /usr/bin/env python3

# This test simultes the initial full sync from a node that is forced to
# consider itself as up-to-date.
# While the resync runs one of the sync-targets promotes and demotes, and the
# cluster is taken down and brought up again.
#
# Up to (including) drbd-9.0.25 this led to an clear-of-bitmap while the
# initial resync runs. It tiggers a log message ("expected n_oos"...) and often
# fails to correctly complete the sync after down and up.

from python import drbdtest
from python.drbdtest import log
from subprocess import CalledProcessError

resource = drbdtest.setup(nodes=3)
resource.disk_options = 'c-max-rate 2M;'
resource.add_disk('10M')

source_n = resource.nodes[0]

resource.nodes.run(['drbdadm', '--force', 'create-md', resource.name])
resource.nodes.new_resource()
resource.nodes.new_minor()
resource.nodes.attach()

source_n.primary(force=True)
source_n.volumes.write(size='100%', bs='64K')
source_n.secondary()

resource.nodes.adjust()
resource.nodes.after_up()

source_pds = resource.peer_devices.from_node(source_n)
target_pds = resource.peer_devices.to_node(source_n)

source_pds.event(r'peer-device .* replication:SyncSource')
target_pds.event(r'peer-device .* replication:SyncTarget')

while True:
    done = source_pds.event(r'peer-device .* done:([0-9.]+)')
    if float(done[0][0]) > 1.0 and float(done[1][0]) > 1.0:
        log(done)
        break

for n in resource.nodes[0:2]:
    n.volumes.write()

resource.down()
resource.up()

source_pds.event(r'peer-device .* replication:Established peer-disk:UpToDate')
target_pds.event(r'peer-device .* replication:Established')

md5sums=[]
for n in resource.nodes:
    md5sum = n.run(['/bin/bash', '-c', 'dd if=%s bs=64k iflag=direct count=150 | md5sum' % (n.volumes[0].device())],
                    return_stdout=True)
    md5sums.append(md5sum)

if len(set(md5sums)) > 1:
    print(md5sums)
    raise RuntimeError("Data differs!")

resource.down()
resource.rmmod()
