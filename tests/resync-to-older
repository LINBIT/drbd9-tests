#! /usr/bin/env python3
### vmshed: { "vms_all": [3], "vms_ci": [3], "drbd_version_min": "9.0" }
#
# Interesting that this issue survived for so long!!
# Consider 3 nodes: A, B, C. (A = Primary, Quorum enabled)
# Down C (gracefully)
# Down B (gracefully, allows A to keep quorum due to "Last man standing"
# Let C resync to A
# Connect B to C and see a split brain up to DRBD 9.0.21!!

# TODO: Execute all possible orders of nodes joining the primary

import time

from python import drbdtest
from python.drbdtest import connections, log, peer_devices

resource = drbdtest.setup_resource(nodes=3)
node_a, node_b, node_c = resource.nodes
resource.resource_options = 'quorum majority; on-no-quorum io-error;'
resource.disk_options = 'c-max-rate 100M;'
volume_number = resource.add_disk('10M', diskful_nodes=[node_a, node_b])

resource.up_wait()

log('* Make up-to-date data available.')
resource.skip_initial_sync()
node_a.primary()

# This was broken up to DRBD 9.1.22/9.2.11 inclusive.
# Fixed by the following DRBD commit:
# f6613104a37c drbd: inherit history UUIDs from sync source when resync finishes
log('* Remove a node that will be left behind')
node_b.down()

log('* Generate a UUID, leaving the removed node behind')
node_a.write(direct=1)

log('* Add disk to diskless node, allowing it to sync to most recent data')
node_c.add_disk(volume_number, '10M')
node_c.volumes[0].create_md()
resource.touch_config()
node_a.adjust()
node_c.adjust()
peer_devices(node_a, node_c).event(r'peer-device .* peer-disk:UpToDate')
node_c.volumes.event(r'device .* disk:UpToDate')

log('* Remove the node that was Primary')
node_a.down()

log('* Bring up node with older data')
node_b.up()

log('* Expect resync from node with added disk to node with older data')
peer_devices(node_c, node_b).event(r'peer-device .* replication:SyncSource')
peer_devices(node_b, node_c).event(r'peer-device .* replication:SyncTarget')
peer_devices(node_c, node_b, bidir=True).event(r'peer-device .* replication:Established')

log('* Restore the full 3-node cluster')
node_a.up()
peer_devices(node_a, bidir=True).event(r'peer-device .* peer-disk:UpToDate')

# No full sync expected after this point
resource.disk_options = 'c-max-rate 250k;'
resource.nodes.adjust()

# This was broken up to DRBD 9.0.21 inclusive.
log('* Create 3 different data generations')
node_a.primary()
connections(to_node=node_a).event(r'connection .* role:Primary')

connections(node_c).disconnect()
node_a.write()

connections(node_b).disconnect()
node_a.write()

log('* Resync from newest to oldest data')
connections(node_c, node_a).connect()
node_c.volumes.event(r'device .* disk:UpToDate')

log('* Start resync to remaining node')
connections(node_b, node_c, bidir=True).connect()
connections(node_b, node_c, bidir=True).event(r'connection .* connection:Connected')

ev_first = peer_devices(node_b, node_c).event(r'peer-device .* replication:SyncTarget .* out-of-sync:(\d+)')
amount_first = int(ev_first[0][0])

# Let the resync run a bit
time.sleep(1.0)

log('* Restart resync to remaining node')
connections(node_b, node_c).disconnect()
connections(node_b, node_c).connect()

ev_second = peer_devices(node_b, node_c).event(r'peer-device .* replication:SyncTarget .* out-of-sync:(\d+)')
amount_second = int(ev_second[0][0])

# This was broken up to DRBD 9.1.15/9.2.4 inclusive.
# Fixed by the following DRBD commit:
# f647d1f7fff6 drbd: set bitmap UUID when setting bitmap slot
if amount_second >= amount_first:
    raise RuntimeError('Second resync ({}) not smaller than first resync ({})'.format(
        amount_second, amount_first))

log('* Allow remaining node to resync from original primary')
connections(node_b, node_a).connect();
connections(node_b, node_a).event(r'connection .* connection:Connected')

node_b.volumes.event(r'device .* disk:UpToDate')

log('* Shut down and clean up.')
resource.down()
resource.cluster.teardown()
