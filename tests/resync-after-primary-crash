#! /usr/bin/env python3
### vmshed: { "vms_all": [3], "vms_ci": null, "drbd_version_min": "9.0" }

# This test creates a 3 node setup with quorum enables and simulates a
# crash of the primary. Then a other node gets promoted, but not a single
# bit is written after promotion.
# The resyncs after re-integration leave one node with an Outdated disk
# frequently. 10 retries seem to be enough to trigger this bug.
#
# This behaviour is reproducable up to (including) 9.0.25

import time
from python import drbdtest
from python.drbdtest import connections, log, peer_devices
from subprocess import CalledProcessError

resource = drbdtest.setup(nodes=3)
resource.resource_options = 'quorum majority;'
resource.add_disk('10M')
resource.up_wait()
log('* Make up-to-date data available.')
resource.skip_initial_sync()
resource.forbidden_patterns.remove(r'connection:NetworkFailure')
resource.forbidden_patterns.remove(r'connection:ProtocolError')
resource.forbidden_patterns.remove(r'connection:BrokenPipe')

[source_n, primary_n, secondary_n] = resource.nodes

def simulate_node_crash(node):
    c = resource.connections.from_node(node)
    c.disconnect(force=True)
    node.down()
    node.drbdadm(['apply-al', resource.name])

    # setting UUID_FLAG_CRASHED_PRIMARY UUID_FLAG_PRIMARY_LOST_QUORUM
    node.run(['drbdmeta', str(node.disks[0].minor), 'v09', node.disks[0].disk, 'internal',
              '--node-id=1', 'set-gi', ':::1::::1', '--force'])

    # Setting some bits in the bitmap, like entries in the activity log would do
    node.dump_md_to_file('/tmp/md.txt')
    node.run(['sed', '-i', '-E',
              's/([0-9]+) times 0x0000000000000000/echo \\"0xFFFFFFFFFFFFFFFF; $((\\1-1)) times 0x0000000000000000; \\"/ge',
              '/tmp/md.txt'])
    node.run(['sed', '-i', '1d', '/tmp/md.txt'])
    node.run(['drbdmeta', str(node.disks[0].minor), 'v09', node.disks[0].disk, 'internal',
              'restore-md', '/tmp/md.txt', '--force'])
    node.run(['rm', '/tmp/md.txt'])

    # Modify the first 4k on the backing device, simulating a "last write" before the crash
    node.fio_file(node.disks[0].disk, drbdtest.fio_write_small_args, direct=1)


def md5sum_first_1k(node):
    return node.run(['/bin/bash', '-c',
                     'dd if=%s bs=1024 iflag=direct count=1 | md5sum' % (node.disks[0].disk)],
                    return_stdout=True)


def test():
    witnessed_resync_from_primary = False
    source_n.primary()
    source_n.volumes[0].write(size='1K', bs='1K', direct=1)
    md5sum = md5sum_first_1k(source_n)

    simulate_node_crash(source_n)
    primary_n.primary()
    source_n.up()

    pd = resource.peer_devices.from_node(secondary_n).to_node(source_n)[0]
    ev1 = pd.event(r'peer-device .* replication:(SyncTarget|Established)')
    if ev1[0][0] == 'SyncTarget':
        ev2 = secondary_n.volumes[0].event(r'device .* disk:(UpToDate|Outdated)')
        pd.event(r'peer-device .* replication:Established')

        if ev2[0][0] == 'Outdated':
            # Expecting a second resync from the primary
            try:
                secondary_n.volumes[0].event(r'device .* disk:Inconsistent', timeout=5)
            except CalledProcessError:
                raise Exception('Sync Target is Outdated after resync')

            secondary_n.volumes[0].event(r'device .* disk:UpToDate')
            witnessed_resync_from_primary = True

    verify_md5sum = md5sum_first_1k(secondary_n)
    if md5sum != verify_md5sum:
        raise Exception('Sync Target data mismatch: %s %s' % (md5sum, verify_md5sum))

    c = resource.connections.from_node(primary_n).to_node(source_n)
    c.event(r'connection .* connection:Connected')

    primary_n.secondary()
    return witnessed_resync_from_primary

#
for i in range(10):
    if test():
        break

resource.down()
resource.teardown()

log(" * Verified that resync from primary happens. It took %d runs." % (i+1))
