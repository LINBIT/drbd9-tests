#! /usr/bin/env python3
### vmshed: { "vms_all": [3], "vms_ci": [3], "drbd_version_min": "9.0" }
#
# This test configures a 3 node system, one primary. It writes a few blocks
# on the primary, and tricks the secondaries to be a different positions
# in the receive stream.
# Then it wants to witness the reconciliation resync between the secondaries
# after the primary disappeared.

import json
import time
from python import drbdtest
from python.drbdtest import connections, log, peer_devices
from python import busywrite
from subprocess import CalledProcessError

NOT = 0
EARLY = 1
LATE = 2
DURING_RESYNC = 3

data_amount = 128


resource = drbdtest.setup_resource(nodes=3)
resource.add_disk('10M')
resource.resource_options = 'quorum majority; on-no-quorum io-error; twopc-timeout 50;'
resource.net_options = 'ping-int 2;'

resource.up_wait()

log('* Make up-to-date data available.')
resource.skip_initial_sync()

primary_n, secondary_slow, secondary_quick = resource.nodes
conn_slow = connections(primary_n, secondary_slow, bidir=True)
conn_quick = connections(primary_n, secondary_quick, bidir=True)

forward_c = connections(secondary_quick, secondary_slow)
backward_c = connections(secondary_slow, secondary_quick)
forward_pd = peer_devices(secondary_quick, secondary_slow)
backward_pd = peer_devices(secondary_slow, secondary_quick)

slow_primary_c = connections(secondary_slow, primary_n)
slow_primary_pd = peer_devices(secondary_slow, primary_n)

to_primary_c = connections(to_node=primary_n)

resource.forbidden_patterns.difference_update([
    r'connection:NetworkFailure',
    r'connection:BrokenPipe',
    r'connection:Timeout',
])

def sectors_received(node, res_name, peer_node_id):
    result = node.run(['drbdsetup', 'status', res_name, '--json'], return_stdout=True)
    status = json.loads(result)[0]
    connection = next(c for c in status['connections'] if c['peer-node-id'] == peer_node_id)
    return connection['peer_devices'][0]['received']

def do_test(with_promote):
    log('* Running variant %d' % (with_promote))

    # Reconnect to primary_n in order to clear received counter.
    to_primary_c.disconnect()
    to_primary_c.connect()
    to_primary_c.event(r'connection .* connection:Connected')

    primary_n.primary()
    to_primary_c.event(r'connection .* role:Primary')

    log('* Blocked network connection to one peer & writing')
    conn_slow.block()

    # BusyWrite lets fio run detached in the background by using setsid
    # the test suite can continue while fio blocks in writing to the drbd dev
    writer = busywrite.BusyWrite(primary_n.volumes[0])
    writer.start(fio_base_args='--iodepth=24 --rw=write --size={}k --direct=1'.format(data_amount))

    # Wait until the "quick" secondary got at least one of those writes
    for i in range(0, 200):
        time.sleep(0.05)
        slow_received = sectors_received(secondary_slow, resource.name, primary_n.id);
        quick_received = sectors_received(secondary_quick, resource.name, primary_n.id);

        if quick_received - slow_received >= data_amount:
            log('* The quick secondary is at least %d KiB writes ahead' %
                       (quick_received - slow_received))
            break
    else:
        raise Exception('No recieve progress!')

    #resource.forbidden_patterns.difference_update([r'connection:NetworkFailure'])
    conn_quick.block()
    log('* Blocked network connection to both secondaries')

    if with_promote == EARLY:
        secondary_slow.primary(wait=False)

    to_primary_c.event(r'connection .* connection:(NetworkFailure|Timeout)')
    to_primary_c.event(r'connection .* connection:Unconnected')

    conn_to_secondaries = connections(primary_n)
    conn_to_secondaries.event(r'connection .* connection:Unconnected')
    #resource.forbidden_patterns.update([r'connection:NetworkFailure'])

    if with_promote == LATE:
        secondary_slow.primary(wait=False)

    evf = forward_pd.event(r'peer-device .* out-of-sync:(\d+)', r'peer-device .* replication:SyncSource')
    evb = backward_pd.event(r'peer-device .* out-of-sync:(\d+)', r'peer-device .* replication:SyncTarget')
    source_amount = int(evf[0][0])
    target_amount = int(evb[0][0])
    if source_amount != data_amount and target_amount != data_amount:
        raise Exception('Neither source(%d) nor destination(%d) see right resync amount(%d)'
                        % (source_amount, target_amount, data_amount))

    if with_promote == DURING_RESYNC:
        secondary_slow.primary(wait=False)

    forward_pd.event(r'peer-device .* replication:Established')
    backward_pd.event(r'peer-device .* replication:Established')

    if with_promote != NOT:
        secondary_slow.secondary(wait=False)

    backing_dev = primary_n.volumes[0].disk
    md5sum_p = primary_n.run(['bash', '-c', 'dd if=%s bs=%dk count=1 iflag=direct | md5sum' % (backing_dev, data_amount)], return_stdout=True)
    md5sum_ss = secondary_slow.run(['bash', '-c', 'dd if=%s bs=%dk count=1 iflag=direct | md5sum' % (backing_dev, data_amount)], return_stdout=True)
    md5sum_sq = secondary_quick.run(['bash', '-c', 'dd if=%s bs=%dk count=1 iflag=direct | md5sum' % (backing_dev, data_amount)], return_stdout=True)

    if md5sum_p != md5sum_ss or md5sum_p != md5sum_sq:
        raise Exception('Md5 sums differ! %s %s %s' % (md5sum_p, md5sum_sq, md5sum_ss))

    primary_n.secondary()
    conn_slow.unblock()
    conn_quick.unblock()

    reconnect_events = [r'peer-node-id:{} .* volume:0 .* peer-disk:UpToDate'.format(secondary_slow.id),
                r'peer-node-id:{} .* volume:0 .* peer-disk:UpToDate'.format(secondary_quick.id)]

    primary_n.event(*reconnect_events)

do_test(with_promote=NOT)
do_test(with_promote=EARLY)
do_test(with_promote=LATE)
do_test(with_promote=DURING_RESYNC)

resource.down()
resource.cluster.teardown()
