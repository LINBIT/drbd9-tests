#! /usr/bin/env python3
### vmshed: { "vms_all": [2], "vms_ci": [2], "vm_tags": ["discard-stat"], "drbd_version_min": "9.2" }

# Before DRBD-9.1.8 drbd issued a discard request for every rs-discard-granularity
# sized block. Ensure it issues only one for every 128MiByte of storage.

# From Documentation/block/stat.txt:
#  0 read I/Os       requests      number of read I/Os processed
#  1 read merges     requests      number of read I/Os merged with in-queue I/O
#  2 read sectors    sectors       number of sectors read
#  3 read ticks      milliseconds  total wait time for read requests
#  4 write I/Os      requests      number of write I/Os processed
#  5 write merges    requests      number of write I/Os merged with in-queue I/O
#  6 write sectors   sectors       number of sectors written
#  7 write ticks     milliseconds  total wait time for write requests
#  8 in_flight       requests      number of I/Os currently in flight
#  9 io_ticks        milliseconds  total time this block device has been active
# 10 time_in_queue   milliseconds  total wait time for all requests
# 11 discard I/Os    requests      number of discard I/Os processed
# 12 discard merges  requests      number of discard I/Os merged with in-queue I/O
# 13 discard sectors sectors       number of sectors discarded
# 14 discard ticks   milliseconds  total wait time for discard requests
#
# It turns out that the discard fields (11 and up) were added with Linux-4.19.
# It was also added to RHEL8 at some point.
#
# Use it on a distro with a kernel new enough.


import os
from python import drbdtest
from python.drbdtest import log
from python import datatools

def number_of_discards(node, vol):
    bd_path = node.run(['readlink', vol.disk], return_stdout=True)
    bd_name = os.path.basename(bd_path)
    stat_content = node.run(['cat', '/sys/block/' + bd_name + '/stat'], return_stdout=True)
    return int(stat_content.split()[11])

resource = drbdtest.setup(nodes=2)
resource.disk_options = 'discard-zeroes-if-aligned yes; rs-discard-granularity 65536; c-max-rate 500M;'
resource.add_disk('1G', thin=True)

node_a, node_b = resource.nodes

node_a.up_wait()
node_a.primary(force=True)
node_a.secondary()

node_b.up_wait()

connection_ba = resource.connections.from_node(node_b).to_node(node_a)
peer_devices_ba = drbdtest.PeerDevices.from_connections(connection_ba)
peer_devices_ba.event(r'peer-device .* replication:SyncTarget')
peer_devices_ba.event(r'peer-device .* replication:Established')

nr_discards = number_of_discards(node_b, node_b.volumes[0])
# 1GiB backend / 128MiB per discard -> 8 discard requests.
if nr_discards > 8:
    raise Exception('More discards than expected nr_discards = {}'
                    .format(nr_discards))

log('* Write different combinations of adjacent intervals')
connection_ba.disconnect()

# Write this pattern, where '0' is zeros, 'x' is data, and ' ' is in sync:
# '0  x 0x0 x0 x 00  x 00  x0 x0 0  x 0x  x0 0  0  x00x  x00x  0  0 0x  x0 x  0 0x 0x  00 x  00 x 0x 0x0 x  '
#
# "randwrite" and "norandommap" are required for "blockalign" to work
# correctly. As a result, the pattern may not be exactly as above. We still
# have good chances of covering all relevant cases.

node_a.fio({},
        direct=1, rw='randwrite', ioengine='libaio', iodepth=16, size='960M', bs='1M',
        blockalign='3M', # scatter blocks
        norandommap=1, # required for blockalign
        buffer_pattern='"x"')

node_a.fio({},
        direct=1, rw='randwrite', ioengine='libaio', iodepth=16, size='960M', bs='1M',
        blockalign='5M', # scatter blocks differently
        norandommap=1, # required for blockalign
        zero_buffers=1)

node_a.fio({},
        direct=1, rw='randwrite', ioengine='libaio', iodepth=16, size='960M', bs='1M',
        blockalign='7M', # scatter blocks differently again
        norandommap=1, # required for blockalign
        zero_buffers=1)

log('* Ensure that a resync is still successful')
connection_ba.connect()
peer_devices_ba.event(r'peer-device .* replication:SyncTarget')
peer_devices_ba.event(r'peer-device .* replication:Established')

datatools.verify_data(resource.nodes)

log('* Shut down and clean up.')
resource.down()
resource.teardown()
