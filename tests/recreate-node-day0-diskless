#! /usr/bin/env python3
### vmshed: { "vms_all": [3], "vms_ci": [3], "drbd_version_min": "9.0" }
#
# This tests mimics the actions that happen when LINSTOR re-creates
# a node that had a thinly provisioned resource.
# I.e. a node goes away and returns later with the day0 current UUID
#
# Only 2 of the 3 nodes have a disk.

from python import drbdtest
from python.drbdtest import MetadataFlag, connections, log, peer_devices
from subprocess import CalledProcessError


def delete_node(node):
    log('* delete {}'.format(node))
    node.down()
    node_a.drbdadm(['del-peer', '{}:{}'.format(resource.name, node.name)])
    node_a.drbdadm(['forget-peer', '{}:{}'.format(resource.name, node.name)])


def recreate_node(node):
    log('* {} is repaired but returns with an empty disk (day0 current UUID)'.format(node))
    # zero out the backing disk
    node.fio_file(node.volumes[0].disk, rw='write', zero_buffers=1, bs='1M')

    node.volumes[0].create_md(max_peers=3)
    node.set_gi(node.volumes[0], '87236C45CB784220',
            flags_set=MetadataFlag.CONSISTENT | MetadataFlag.WAS_UP_TO_DATE)

    # connect to node again
    node_a.adjust()
    # bring node up
    node.adjust()
    node.volumes.event(r'device .* disk:UpToDate')
    peer_devices(node_a, node).event(r'peer-device .* peer-disk:UpToDate')


resource = drbdtest.setup_resource(nodes=3)
diskful_nodes = resource.nodes[:2]
node_a, node_b, node_c = resource.nodes
resource.add_disk('100M', max_peers=3, diskful_nodes=diskful_nodes) # That is one spare to keep day0 bitmap

for n in diskful_nodes:
    n.set_gi(n.volumes[0], '87236C45CB784220',
            flags_set=MetadataFlag.CONSISTENT | MetadataFlag.WAS_UP_TO_DATE)

log('* All nodes on same current UUID, waiting for them to connect.')
resource.up()
resource.peer_devices.event(r'peer-device .* peer-disk:(UpToDate|Diskless)')

log('* form a new current UUID that differs from the day0 UUID')
node_a.write()


log('*** delete and recreate diskful node while keeping diskless node')
delete_node(node_b)

log('* remaining nodes form new current UUID')
node_a.write()

recreate_node(node_b)


# The case below was broken up to DRBD 9.1.14/9.2.3 inclusive.
# With these versions, the case above is also broken in theory, but the issue does not cause any noticeable problems.
# Fixed by the following DRBD commit:
# d8f5cab5b561 drbd: do not copy zero UUID from diskless peer when freeing peer slot
log('*** delete and recreate diskful node, deleting diskless node as well')
delete_node(node_b)

delete_node(node_c)
resource.remove_node(node_c)

log('* remaining nodes form new current UUID')
node_a.write()

recreate_node(node_b)

resource.add_node(node_c)
resource.nodes.adjust()
connections(node_c, bidir=True).event(r'peer-device .* peer-disk:(UpToDate|Diskless)')


# The case below was broken up to DRBD 9.1.15/9.2.4 inclusive.
# Fixed by the following DRBD commit:
# 773c5cdc7723 drbd: reset diskless peer slot to day0 UUID when forgetting
log('*** delete diskless node, then delete and recreate diskful node')
delete_node(node_c)
resource.remove_node(node_c)

delete_node(node_b)

log('* remaining nodes form new current UUID')
node_a.write()

recreate_node(node_b)

log('* Shut down and clean up.')
resource.down()
resource.cluster.teardown()
