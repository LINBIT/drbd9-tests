#! /usr/bin/env python3
### vmshed: { "vms_all": [2], "vms_ci": [2] }
# Pass this script a list of host names to use as the test nodes.

import time
import random
from python import drbdtest
from python.drbdtest import log
from subprocess import CalledProcessError

M_REATTACH=1
M_RECREATE_MD=2

def test_writes(fault_type):
    log('* Writing on primary (autopromote) secondary should detach')
    secondary_n.set_fault_injection(vol, drbdtest.DF_DATA_WRITE)
    primary_n.run(['dd', 'if=/dev/zero', 'of=%s' % (vol.device()) , 'bs=4096', 'oflag=direct', 'count=1'])

    secondary_n.volumes.event(r'device .* disk:Failed')
    secondary_n.volumes.event(r'device .* disk:Diskless')
    primary_n.peer_devices.event(r'peer-device .* peer-disk:Failed')
    primary_n.peer_devices.event(r'peer-device .* peer-disk:Diskless')

    log('* down/up secondary, check resync')
    secondary_n.disable_fault_injection(vol)
    secondary_n.down()
    secondary_n.up()
    secondary_n.volumes.event(r'device .* disk:Inconsistent')
    secondary_n.peer_devices.event(r'peer-device .* replication:SyncTarget .* out-of-sync:4096')
    secondary_n.volumes.event(r'device .* disk:UpToDate')
    primary_n.peer_devices.event(r'peer-device .* replication:SyncSource .* out-of-sync:4096')
    primary_n.peer_devices.event(r'peer-device .* replication:Established')
    # Expect 4k resync. That ensure it is a bitmap-based resync and not a full resync

    primary_n.primary()
    primary_n.secondary()
    # This primary/secondary seems pointless, but triggers what was fixed with
    # b5f48453e1b95d7a53bee1820f2ff1d5b23cb6f1 in drbd9

    log('* Testing IO error on primary (autopromote), should detach')
    primary_n.set_fault_injection(vol, drbdtest.DF_DATA_WRITE)
    primary_n.run(['dd', 'if=/dev/zero', 'of=%s' % (vol.device()) , 'bs=4096', 'oflag=direct', 'count=1'])

    primary_n.volumes.event(r'device .* disk:Failed')
    primary_n.volumes.event(r'device .* disk:Diskless')
    secondary_n.volumes.event(r'peer-device .* peer-disk:Failed')
    secondary_n.volumes.event(r'peer-device .* peer-disk:Diskless')

    log('* down/up primary, check resync')
    primary_n.disable_fault_injection(vol)
    primary_n.down()
    primary_n.up()
    primary_n.volumes.event(r'device .* disk:Inconsistent')
    primary_n.peer_devices.event(r'peer-device .* replication:SyncTarget .* out-of-sync:4096')
    primary_n.volumes.event(r'device .* disk:UpToDate')
    secondary_n.peer_devices.event(r'peer-device .* replication:SyncSource .* out-of-sync:4096')
    secondary_n.peer_devices.event(r'peer-device .* replication:Established')
    # Expect 4M resync. One AL-extent. That ensure it is a bitmap-based resync and not a full resync

def test_tracking_writes(mode):
    # check the resync of blocks while the disk on the primary is away. Do it in a way the
    # AL does not get applied, so that we verify that the diskfull secondary tracked the
    # changes.
    log('* verify resync amount %s' % ("after reattach" if mode == M_REATTACH else "with replaced backing disk"))
    primary_n.detach()
    primary_n.primary()
    primary_n.run(['dd', 'if=/dev/urandom', 'of=/tmp/data', 'bs=1024', 'count=12'])
    md5sum = primary_n.run(['md5sum', '/tmp/data'], return_stdout=True)
    primary_n.run(['dd', 'if=/tmp/data', 'of=%s' % (vol.device()), 'bs=1024', 'oflag=direct', 'count=12', 'seek=10216'])
    primary_n.secondary()
    if mode == M_RECREATE_MD:
        primary_n.drbdadm(['create-md', '--force', resource.name])

    primary_n.run(['dd', 'if=/dev/zero', 'of=%s' % (vol.disk_lv), 'bs=1024', 'oflag=direct', 'count=12', 'seek=10216'])
    primary_n.attach()

    primary_n.volumes.event(r'device .* disk:Inconsistent')
    if mode == M_REATTACH:
        primary_n.peer_devices.event(r'peer-device .* replication:SyncTarget .* out-of-sync:12')
        secondary_n.peer_devices.event(r'peer-device .* replication:SyncSource .* out-of-sync:12')
    elif mode == M_RECREATE_MD:
        primary_n.peer_devices.event(r'peer-device .* replication:SyncTarget .* out-of-sync:12248')
        secondary_n.peer_devices.event(r'peer-device .* replication:SyncSource .* out-of-sync:12248')
    primary_n.run(['dd', 'if=%s' % (vol.device()), 'of=/tmp/data', 'bs=1024', 'iflag=direct', 'count=12', 'skip=10216'])
    verify_md5 = primary_n.run(['md5sum', '/tmp/data'], return_stdout=True)
    if verify_md5 != md5sum:
        raise Exception("Got back wrong data! %s %s" % (md5sum, verify_md5))
    primary_n.volumes.event(r'device .* disk:UpToDate', timeout=300) # Need to wait for 10MB full sync
    secondary_n.peer_devices.event(r'peer-device .* replication:Established')


# main

random.seed()
resource = drbdtest.setup(min_nodes=2, max_nodes=2)
resource.add_disk('10M')
resource.up_wait()

log('* Make up-to-date data available.')
resource.skip_initial_sync();

primary_n = random.choice(resource.nodes)
secondary_n = resource.nodes.difference([primary_n])[0]
vol = resource.volumes[0]

resource.forbidden_patterns.difference_update([r'disk:Failed', r'peer-disk:Failed'])

for fault_type in [drbdtest.DF_DATA_WRITE, drbdtest.DF_META_WRITE]:
    test_writes(fault_type)

test_tracking_writes(M_REATTACH)
test_tracking_writes(M_RECREATE_MD)

# TODOs: read errors, IO errors during resync, allocation errors

log('* Shut down and clean up.')
resource.down()
resource.teardown()
