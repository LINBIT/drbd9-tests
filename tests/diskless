#! /usr/bin/env python3

# Pass this script a list of host names to use as the test nodes.

from python import drbdtest
from python.drbdtest import verbose
from subprocess import CalledProcessError

resource = drbdtest.setup(min_nodes=3, max_nodes=5)

diskful_nodes = resource.nodes[1:]
resource.add_disk('10M', diskful_nodes=diskful_nodes)

resource.up_wait()

# NOTE: drbdadm primary --force currently fails for new data on diskless nodes:
# drbd doesn't know which node to turn UpToDate.  We could implement a "drbdadm
# uptodate" or similar command that turns a node UpToDate without making in
# primary.  Until then, we have to make one of the diskful nodes primary and then
# secondary again to make it UpToDate.

verbose('* Make up-to-date data available.')
resource.skip_initial_sync();

nfnr = 0
md5_table = []
def create_file(node, count):
    global nfnr, md5_table
    node.run(['dd', 'if=/dev/urandom', 'of=/mnt/1/f_%d' % (nfnr) , 'bs=4096',
              'count=%d' % (count), 'conv=fsync'])
    md5_table.insert(nfnr, node.run(['md5sum', '/mnt/1/f_%d' % (nfnr)], return_stdout=True))
    nfnr += 1

def verify_files(node):
    global md5_table
    node.run(['/bin/bash', '-c', 'echo 1 >/proc/sys/vm/drop_caches'])
    for fnr, write_md5 in enumerate(md5_table):
        read_md5 = node.run(['md5sum', '/mnt/1/f_%d' % (fnr)], return_stdout=True)
        if write_md5 != read_md5:
            raise RuntimeError("Data corruption found file f_%d" % (fnr))

diskless_node = resource.nodes.diskless[0]
dev_name = diskless_node.volumes[0].device()


verbose('* Test loosing all but one server, and reintegrating them in the same order')
diskless_node.run(['mkfs', '-t', 'ext4', dev_name])
diskless_node.run(['mkdir', '-p', '/mnt/1'])
diskless_node.run(['mount', dev_name, '/mnt/1'])
create_file(diskless_node, 100);

nr_nodes = len(resource.nodes)

# Disconnect nearly all servers
for n in resource.nodes.diskful[1:]:
    verbose('* taking downing %s' % (n.name))
    n.down()
    create_file(diskless_node, 100);

# Reintegrate ony by one
up_so_far=drbdtest.Nodes([resource.nodes.diskful[0]]);
for n in resource.nodes.diskful[1:]:
    n.new_resource()
    n.new_minor()
    n.new_peer()
    n.peer_device_options()
    n.new_path()
    n.attach()

    verbose('* re-integrating %s' % (n.name))
    # TODO Sometimes to diskles first, sometimes to the others first

    cs = drbdtest.Connections() # Connections from n to all other diskful_nodes
    for n2 in resource.nodes.diskful.difference([n]):
        cs.add(drbdtest.Connection(n, n2))
    cs.connect()

    # Wait for the connections to connect and my disk to become Inconsistent. Arbitrary order!
    regexs = []
    for n2 in up_so_far:
        regexs.append('connection .* peer-node-id:%d .* connection:Connected' % (n2.id))

    regexs.append('device .* volume:%d .* disk:Inconsistent' % n.volumes[0].volume)
    n.event(*regexs)

    create_file(diskless_node, 10);     # Write something during resync

    # Resync finished
    n.event(r'response helper .* helper:after-resync-target')

    cs = drbdtest.Connections()
    cs.add(drbdtest.Connection(n, diskless_node))
    cs.connect()
    cs.event(r'connection .* connection:Connected')
    n.volumes.event(r'device .* disk:UpToDate')

    # Waiting for the reverse is necessary as well, otherwise the following
    # disconnect might fail
    cs = drbdtest.Connections()
    cs.add(drbdtest.Connection(diskless_node, n))
    cs.event(r'connection .* connection:Connected')

    cs = drbdtest.Connections() # Connections from diskless_node to all other diskful_nodes
    for n2 in up_so_far:
        cs.add(drbdtest.Connection(diskless_node, n2))
    cs.disconnect()
    verify_files(diskless_node)
    cs.connect()
    cs.event(r'connection .* connection:Connected')
    up_so_far.event(r'device .* disk:UpToDate')

    up_so_far.add(n)

# Early Exit
diskless_node.run(['umount', '/mnt/1'])

verbose('* Writing from diskless node.')

diskless_node.primary()
diskless_node.fio(section='write')
diskless_node.secondary()

verbose('* Verifying on diskful node.')
diskful_nodes[0].fio(section='verify')

verbose('* Disconnecting nodes.')
diskless_node.primary()
resource.connections. \
    from_node(diskless_node). \
    to_nodes(diskful_nodes[:-1]). \
	disconnect()

try:
    diskless_node.disconnect(diskful_nodes[-1])
except CalledProcessError:
    pass
else:
    raise CalledProcessError("'%s'.disconnect('%s') unexpectedly succeeded" %
	(diskless_node, diskful_nodes[-1]))

try:
    diskful_nodes[-1].disconnect(diskless_node)
except CalledProcessError:
    pass
else:
    raise CalledProcessError("'%s'.disconnect('%s') unexpectedly succeeded" %
	(diskful_nodes[-1], diskless_node))

diskless_node.secondary()

# FIXME: If we don't wait for all nodes to realize that diskless_node has
# become secondary, drbd will fail with the following error:
#   State change failed: (-2) Need access to UpToDate data
# Even this wait often isn't enough to prevent this error, though -- this needs
# to be debugged.
resource.connections. \
    to_node(diskless_node). \
	event(r'connection .* role:Secondary')

# FIXME: Simulate network outage

try:
    diskless_node.run(['sleep', '0.5'])
    diskless_node.run(['udevadm', 'settle', '--timeout=2'])
except:
    pass

verbose('* Shut down and clean up.')
resource.down()
resource.rmmod()
