#! /usr/bin/env python3
### vmshed: { "vms_all": [3], "vms_ci": [3], "drbd_version_min": "9.0" }
#
# This test configures a 3 node system, isolates one node (A), writes something
# on the remaining partition (B & C). Let the isolated node resync with one of
# the nodes having the new data (B). Then, make it primary while it is sync
# target. Wait until resync finishes, connect it to the other node (C)
# from the "new data partition".

from python import drbdtest
from python.drbdtest import connections, log, peer_devices

resource = drbdtest.setup_resource(nodes=3)
resource.disk_options = 'c-max-rate 1M;'
resource.add_disk('10M')
resource.up_wait()

log('* Make up-to-date data available.')
resource.skip_initial_sync()

[node_a, node_b, node_c] = resource.nodes

log('* Isolating %s' % (node_a.name))
connections(node_a).disconnect()

log('* New data on %s %s' % (node_b.name, node_c.name))
node_b.volumes.write(size='9M', bs='1M')

log('* Resync from %s to %s' % (node_b.name, node_a.name))
connection_ab = connections(node_a, node_b)
connection_ab.connect()

peer_device_ab = peer_devices(node_a, node_b)
# wait until events2 reports >= 20 % done
peer_device_ab.event(r'peer-device .* done:([2-9][0-9][0-9.]+)')

log('* Become primary on %s' % (node_a.name))
node_a.primary()
peer_device_ab.peer_device_options("--c-max-rate=60M")
node_a.volumes.write()

node_a.volumes[0].event(r'device .* disk:UpToDate')

connection_ac = connections(node_a, node_c)
connection_ac.connect()

log('* Ensure resync from %s to %s' % (node_a.name, node_c.name))
connection_ca = connections(node_c, node_a)
connection_ca.event(r'connection .* connection:Connected')
connection_ac.event(r'connection .* connection:Connected')
peer_device_ca = peer_devices(node_c, node_a)
[[repl_state]] = peer_device_ca.event(r'peer-device .* replication:([^ ]+)')
if repl_state != 'WFBitMapT':
    raise RuntimeError("Expected a resync here but got %s" % (repl_state))

peer_device_ca.peer_device_options("--c-max-rate=60M")
peer_device_ca.event(r'peer-device .* replication:Established')

log('* Shut down and clean up.')
resource.down()
resource.cluster.teardown()
