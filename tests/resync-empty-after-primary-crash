#! /usr/bin/env python3

# This test creates a 3 node setup with quorum enables and simulates a
# crash of the primary. Then a other node gets promoted, but not data
# is written after promotion.
# The resyncs after re-integration leave one node with an Outdated disk
# frequently. 10 retries seem to be enough to trigger this bug.
#
# This behaviour is reproducable up to (including) 9.0.25

import time
from python import drbdtest
from python.drbdtest import verbose

resource = drbdtest.setup(nodes=3)
resource.resource_options = 'quorum majority;'
resource.add_disk('10M')
resource.up_wait()
verbose('* Make up-to-date data available.')
resource.skip_initial_sync()
resource.forbidden_patterns.remove(r'connection:NetworkFailure')

[source_n, primary_n, secondary_n] = resource.nodes

def test():
    source_n.primary()
    source_n.volumes[0].write(count=1, bs=1024, flags=['oflag=direct'])
    time.sleep(0.8)

    c = resource.connections.from_node(source_n)
    c.disconnect(force=True)
    source_n.down()
    # setting UUID_FLAG_CRASHED_PRIMARY UUID_FLAG_PRIMARY_LOST_QUORUM
    source_n.run(["drbdmeta", source_n.disks[0].minor, "v09", source_n.disks[0].disk, "internal",
                  "--node-id=1", "set-gi", ":::1::::1", "--force"])
    primary_n.primary()
    source_n.up()

    pd = resource.peer_devices.from_node(secondary_n).to_node(source_n)[0]
    pd.event(r'peer-device .* replication:SyncTarget')
    ev = secondary_n.volumes[0].event(r'device .* disk:(UpToDate|Outdated)')
    pd.event(r'peer-device .* replication:Established')
    if ev[0][0] != 'UpToDate':
        raise Exception('Sync Target %s after resync.' % ev[0][0])

    primary_n.secondary()

#
for i in range(10):
    test()

resource.down()
resource.rmmod()
