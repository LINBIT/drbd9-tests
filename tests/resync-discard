#! /usr/bin/env python3
### vmshed: { "vms_all": [2], "vms_ci": [2], "drbd_version_min": "9.1", "variants": ["tcp", "rdma", "zfs"] }

# Before DRBD-9.1.6 this caused a fully allocated
# backing volume on the resync target node after
# the resync

import json
import os

from python import drbdtest
from python.drbdtest import connections, log, peer_devices


resource = drbdtest.setup(nodes=2)
resource.disk_options = 'discard-zeroes-if-aligned yes; rs-discard-granularity 65536; c-max-rate 10M;'
resource.add_disk('11M', thin=True)

node_a, node_b = resource.nodes
dev_name = node_a.volumes[0].device()

node_a.up_wait()
node_a.primary(force=True)
node_a.secondary()

node_b.up_wait()

connection_ba = connections(node_b, node_a)
peer_devices_ba = drbdtest.PeerDevices.from_connections(connection_ba)
peer_devices_ba.event(r'peer-device .* replication:SyncTarget')
peer_devices_ba.event(r'peer-device .* replication:Established')

data_percent_a = node_a.disk_tools.fill_percentage(node_a.volumes[0].disk)
data_percent_b = node_b.disk_tools.fill_percentage(node_b.volumes[0].disk)

if data_percent_a != data_percent_b:
    raise Exception("node_a data_percent {} while node_b shows {}"
                    .format(data_percent_a, data_percent_b))

log('* Shut down and clean up.')
resource.down()
resource.teardown()
